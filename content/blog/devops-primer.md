+++
title = 'Devops Primer'
date = 2024-08-24T07:49:21-05:00
+++

For years now, I've worked with software running on Docker and Kubernetes.  I've written a few Dockerfiles here and there and even one or two Docker Compose files, but I've never sat down and actually studied these technologies.  Recently, when debugging an issue for work, I felt that there were some gaps in my knowledge of Docker that would help me resolve my issue faster.  I sat down with the intention of going through the basics of Docker, Docker Compose and Kubernetes to fill gaps and ensure I have a working knowledge of each of these essential DevOps technologies.

# The Learning Path
I knew I wanted to learn Docker, Docker Compose, and Kubernetes, and I wanted something very hands on since that's how I enjoy learning.  Luckily, I was able to find a YouTuber (TechWorld With Nana) who had a free course on each one of these technologies.  I won't be reiterating everything here, so if you're interested you should also go watch the videos.  It's well worth the time investment.
[Docker Crash Course for Absolute Beginners](https://www.youtube.com/watch?v=pg19Z8LL06w)
[Ultimate Docker Compose Tutorial](https://www.youtube.com/watch?v=SXwC9fSwct8)
[Kubernetes Crash Course for Absolute Beginners](https://www.youtube.com/watch?v=s_o8dwzRlu4)

# Background
When I first started my career, Docker had just come, but had not been heavily adopted yet.  When I joined teams you were given a Microsoft Word file with all the instructions for setting up your development environment - you'd have to go to Oracle's website, download the database to install, ensure you have Java so it can run, setup all your environment variables, etc.  You had to do this for every development dependency and it was an absolute pain.  Once you got everything setup, you could start coding.  Software engineers at this time were **only** responsible for their code.  Once the code was done, you handed it over to a server team (completely separate from your own team).  This server team would also have to go through the word document and manually install and configure all the dependencies for your code to run.  If anything was not documented correctly, you'd have to go back and forth until everything was configured properly.  Release times were SLOW - maybe a few times a year.

# Enter Docker
With the rise of Docker, many of these issues were solved.  Docker gives us a way to run applications in isolated containers, which ensures every developer and the server teams are running and configuring dependencies and application code in the same way.  With Docker, you just need to run a `docker run` command, possibly with some configuration options.  This will create a container with the dependency that you can use, just like if you installed it on your local machine.  As long as everyone runs the same Docker commands you can be confident that everyone's machine is setup the same.
As you can see, Docker is incredibly powerful, but how does it work?  Docker is based on `images`.  These are portable artifacts that are packaged up and ready for download. They can be something as simple as a Linux environment or they can be higher level like a database, a message queue, or even a linux environment with a programming language runtime built into it (with these you'd normally be adding your application source code to it and running it as your application container).  An image is just that through - the artifact.  If you actually want to run it you must create a container, a running instance of an image.  Lucky this is very simple: `docker run redis` where redis is the name of the image you want to run.  After this, you would have an container for Redis on your machine.  This relationship reminds me  a lot of the relationship between classes and objects.  Just as a class is a blueprint to create an instance of an object, an image is a blueprint to create an instance of a container.  There is much more to learn about Docker such as exposing ports, so your applications can actually interact with the container, creating volumes so you can persist data even if your container is deleted, and networking so containers can talk to each other by container name.  Nana goes over all of these in her course, so if you feel your knowledge is lacking in any of these I highly recommend starting there.

# Docker Compose
Once you start working with Docker more, you'll quickly realize a problem - you're running ALOT of containers and you have to remember the commands for all of them.  Maybe you could create a Makefile with the commands so you can just run `make run-app` to start everything up, but a better way is to use Docker Compose.  Docker Compose is a declarative way to define all the containers your application needs to run in a single YAML file, with all the configuration necessary.  Once you have the YAML file setup, all you have to do is run `docker-compose up` and your application and all it's dependencies will start, while  `docker-compose down` will tear them all down.  Additionally, you can configure the startup order of your containers, so you don't have failures from containers starting before their dependencies are ready.    Docker-Compose will also handle creating a custom network for your application, so all containers can talk to each other just by using the container name as opposed to IP addresses.  I discovered this website [composerize](https://www.composerize.com/), which will let you paste in your `docker run` commands and will output a compose file for you to use.  Docker Compose is a great tool in your belt for local development, however it does run into problems if you were try to deploy a large multi container app in production because it simply wasn't built for massive scale.  Additionally, while it's great for starting and stopping multi container apps, it doesn't help you manage your containers .  For production, you'd want something more full featured like Kubernetes.

# Kubernetes
Kubernetes, also commonly called k8s because there are 8 letters between the "k" and the "s", is a framework for orchestrating your containers.  It provides many features for managing containerized applications.  
Some of these features include:
- horizontal scaling - we can tell k8s how many instances of a certain pods we want running and it will ensure we always have enough
- autoscaling - k8s can respond to traffic to increase or decrease the the number of pods based on load
- self-healing - k8s will restart a pod that dies, but it will also monitor health checks and if a pod becomes unhealthy it will automatically restart it

Kubernetes has a lot of associated terminology that can be confusing at first glance.  This is what always kept me away from wanting to learn more about it, but Nana does a great job of explaining it in her videos.  Essentially,  Kubernetes has 2 main parts - **nodes** and **the control plane**.  **Nodes** are the individual servers that will run your containerized apps and dependencies, while the **control plane** is the brains of Kubernetes that keeps track of the state of everything running and makes decisions for orchestration between the nodes (like if things need restarted or autoscaling needs to occur).  The control plane and all the nodes together are called a K8s **cluster**. The part of the control plane you will work with the most is the **API server**, which provides a UI, API and CLI interface for engineers interact with the cluster to configure it.   Each node runs the **kubelet** process, which is what allows the Control Plane to communicate to the nodes.

![k8s-architecture](..images/k8s-architecture.png)

Every node in k8s runs one or more **pods**.  A pod is a group of one or more containers run together.  Typically, a pod is one application container and possibly side-car containers, which are helper containers that run alongside your application to provide additional functionality like log and metrics collectors. Each pod will get it's own internal IP address, but since pods are ephemeral, your application cannot directly talk to this IP address.  Instead you setup a **Service**, which creates a permanent IP address for all pods that are running that application and the k8s control plane will load balance between them.  This way if you had 1 pod or 12 for your application , it doesn't matter - k8s handles routing the traffic to the correct place.  The main component you will be working with to manage your pods is **Deployments**.  With deployments you describe what container image you want to run, the ports, env variables, etc, just like with docker-compose, but you additionally specify the number of replicas, so that k8s can monitor and scale your application. This is just the tip of the iceberg with k8s.  There's many other components to learn about, like ConfigMaps, Secrets, StatefulSets and more, but this is enough to get started with a basic application.

So how exactly does configure and manage a k8s cluster?  Well very similar to docker-compose, K8s is managed via declarative configuration files, written in YAML, and then applied to the cluster.  This keeps your configuration version controlled and easily replicable. The TechWorld with Nana course will walk you through writing all the files necessary to get an application running on Kubernetes and additionally will show you how to setup MiniKube to run a k8s cluster locally on your computer.

# What's next?
Just watching the 3 videos will ensure you have a great foundational knowledge or Docker, Docker Compose and Kubernetes.  To take this a step further I'd like to learn more about using Kustomize to manage the Kubernetes components, Istio for service mesh, Helm charts, and running log and metric collectors in sidecars.
